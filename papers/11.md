## Going Deeper with Convolutions. 

#### Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. 2014. 

The authors present a new architecture named GoogLeNet which is based on using a sparsely connected architecture in order to avoid computational bottlenecks and improve computational efficiency over the entire network as they go deeper and wider. The sparsely connected architecture (called Inception modules) are based on the work of Arora et. al. which suggests the use of a layer by layer construction of the network using the inception modules based on correlation statistics of the previous layer when clustering the units in order to define the units of the next layer. Dimension reduction using 1x1 convolutional filters and projections is done to accommodate layers where the computational requirements spike. The authors provide details on their best performing model as well as the design considerations when submitting for the ILSVRC 2014 which GoogLeNet won.

Discussion:

1. Rather than a question, one criticism I have of this paper is that the authors have made a lot of design decisions for convenience rather than providing sound scientific reasoning. Even when they do, it is either empirical or based on theoretical work, not both. I wonder if the authors have published a follow up paper where they discuss why their other models and decisions failed.

2. The use of the 1x1 projections has not been elaborated upon. I believe this is the 1x1 convolution done on the 3x3 max pooling layer. Can some light be shed upon what is happening in this part of the Inception module?

3. The authors used a deeper and wider network, which gave poorer results, in conjunction with the ensemble. This was also done in AlexNet. How important is using an ensemble of Deep Networks for object classification? Has there been any study on this?