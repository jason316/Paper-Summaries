## ImageNet Classification with Deep Convolutional Neural Networks. 

#### Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton. NIPS 2012. 

The paper discusses the design decisions made to create a Convoutional Neural Network with the objective of minimizing overfitting, that was capable of winning the ImageNet LSVRC 2010 and 2012. The main decisions, in order of importance are, Rectified Linear Units which help speed up training, a 2 GPU implementation with an innovative step of allowing only certain layers (every layer except the 2nd, 4th and 5th) to communicate across GPU, local response normalization which basically causes neurons to contest for larger activities, and overlapping pooling in contrast to local pooling. To reduce overfitting, the training images were augmented by patching, translating and mirroring each training image into a new training image of size 224x224, applying PCA over the RGB values of the training images over the entire set and multiplying each image with the eigenvectors weighted by the randomly scaled eigenvalues, and using Dropout regularization which involves randomly turning off neurons so as to simulate different architectures. Results of their submissions are provided.

Discussion:

1. Can the process of Local Response Normalization be elaborated upon? I don't seem to understand it completely.
2. What was the guiding decision behind which layers would be able to communicate across GPUs? That does not seem to have been given enough attention.