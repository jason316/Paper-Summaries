## Show and Tell: A Neural Image Caption Generator

#### Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan
#### 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)

This paper proposes using advances in machine translation using sequence to sequence generation networks to generate natual language descriptions of images. The primary contribution is the sequence generation model, in this case an LSTM, which is fed the image representation of the image via a CNN model, thus projecting the image and the caption/description into the same embedding space. The LSTM is trained the minimize the sum of the negative log likelihood of the words of the caption, and is tasked with generating a sequence of words from a pre-defined dictionary where each word is conditioned on the image and the previous words.

The authors provide experiments across a multitude of datasets and report the BLUE-4 scores since it is the standard metric for machine translation and thus corresponds directly in this task. The results show that caption generation yields qualitatively and quantitatively better and more diverse results than non deep learning based approaches and ranking-based approaches. Empirical results are shown of 5 different datasets, leading to believe that the model is indeed powerful enough to learn good semantic representations.

#### Strengths

1. The primary motivation for this paper comes from the use of sequence generation models for the task of machine translation. There the task was to convert a sequence of words in one language to a sequence of words in another language. Thus this model leverages proven advances in other domains to solve a hard problem.
2. For the task of image captioning, the authors replace the learned representation of the source sentence with a learned representation of the image in question, thus nicely fitting the problem into an existing framework. 
3. The paper demonstrates the power of generation over standard retrieval or ranking techniques that were the state of the art at the time of publication. Although their formulation is similar to the work from Mao et. al. as mentioned in the related work section, they claim their use of the LSTM RNN model leads to improved results that are much closer to human-like performace as per the BLEU-4 metric. 
4. The model is conceptually simple and common issues such as overfitting and human based qualitative analysis are tackled, leading to an approach that would seem to work well in the real world.
5. Emprical experiments on 4 different datasets as well as human rating helps corroborate their contributions.

#### Weaknesses

There are a few obvious as well as subtle deficiencies in the paper:
1. The model configuration was not experimented with. It would have been interesting to see a comparison with a variety of CNN models as well as fixing the CNN models with different RNN models to examine how much a good representation of the image is important for this task.
2. There is no ablation study on the model. We do not see the effectiveness of the CNN model, versus the word embedding model, versus the LSTM model independently. This study alongwith the previous point would have helped corroborate their argument for why their choice of an LSTM was well founded.
3. No comparisons against the work of Kiros et. al. and Karpathy et. al. The argument can be made that the work of Mao et. al. aka the m-RNN model provides the best apples-to-apples comparison, but the authors have provided comparison metrics against non deep methods, thus for the sake of completion, having comparisons to other deep approaches would have been nice.
4. The authors seem to have been in 2 minds about reporting about CIDEr and METEOR. While they report these metrics for baseline MSCOCO tests, they only report BLEU-4 metrics across other approaches. This weakness could be attributed to the non-availability of the models used for these approaches and hence should be taken with a grain of salt.

#### Reflections

Since the approach of this paper is a conceptually simple model that is fully data-driven, it is highly amenable to build further upon and improve the results. Future research directions could be improving on more sophisticated metrics such as CIDEr and building from that. The task of paragraph generation could be tackled which would require a high degree of diversity in the modelâ€™s generated sequences. Also, there has been work on using bi-directional LSTMs to encode the context of the caption during training. Leveraging such sophisticated techniques could prove useful in improving the model and improving its performance in not just the automated metrics but also the human ratings.
