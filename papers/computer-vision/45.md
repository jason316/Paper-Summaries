## Show, Attend and Tell: Neural Image Caption Generation with Visual Attention

#### Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio
#### 2015 Proceedings of the 32nd International Conference on Machine Learning

This paper presents a framework for integrating the technique of visual attention which is a means to encourage a deep network to focus on certain area of an input image more when generating an output, which in this case is a word in a sequence. The authors utilize existing work on Neural Image Captioning and illustrate the use a novel mechanism to incorporate attention on certain visual features extracted from a lower convolution layer rather than a fully connected layer. 
The attention is modeled by utilizing a parameter (alpha) which can be modeled as either a probability distribution over the importance of the features or as weights. Depending on the perception of the attention parameter, the visual attention framework can be hashed as either stochastic "hard" attention or deterministic "soft" attention respectively. Hard attention is a sampling based mechanism that forces the network to pick one feature vector over others, while soft attention is an expectation over the feature vectors (aka a weighted sum). In either case, the attention parameter is used to generate a context vector for the LSTM at each time step, so the LSTM is effectively looking at different parts of the image while generating the next term of the sequence. In both cases, they illustrate the corresponding objective function formulation which helps the network learn both the parameters as well as the best caption sequence.
The authors show the results of their proposed technique over 3 datasest, Flickr8K, Flickr30K and MS COCO. Using reported BLUE and METEOR scores, they quantitatively validate that an attention based model is superior to the then state-of-the-art techniques.

### Strengths

- Just as Batch Normalization is a general technique for improving gradient propagation in deeper networks, the biggest strength of this paper is the generic capabilities of the visual attention model and its wide applicability. From the formulation of attention in this paper, it seems that visual attention could be easily modeled as a layer that takes the CNN's activations and learns the best weights (aka alpha parameters) to model visual attention, thus making it trivial to add to new applications that would benefit from visual attention.
- The soft attention is much more interesting than the hard attention because there is no effective loss of information and the authors show mathematically that the soft attention can easily be calculated as an expectation of the feature/annotation vectors over the attention parameters and this corresponds to optimizing the overall log likelihood of the sequence generated since the softmax of the projections of the LSTM over this expectation is approximately the same as the normalized weighted geometric mean which is the overall objective. This gives researchers strong guarantees about the performance of deep network, something that is a very hot topic in theoretical machine learning.
- Interpretibility has always been a challenge in Deep Learning, with whole PhD theses forming from this problem. The attention mechanism gives researchers a very intuitive form of visualization to help debug failure modes, similar to classical feature representations such as HoG.

### Weaknesses

- The authors do not explain the choice of using the 4th convolutional layer of VGG, or which VGG model for that matter. One could imagine that multi-layer fusion via skip connections or top-down modulation could help provide richer semantics for many applications in addition to caption generation.
- While both attention mechanisms have their merits, the authors fail to illustrate the computational complexity of the hard attention model. While the soft attention model is straightforward to train via back-propagation, the hard attention mechanism uses Monte Carlo sampling which is known to be computationally expensive from Probabilistic Graphical Models literature. It is not clear from the paper if there is a trade-off between the hard and soft attention mechanisms, which is important since it is what achieves the best results across all the datasests on the BLEU[1-4] metrics.
- A quick quantitative analysis between VGG and Alexnet as their base feature extractors would have been a nice to have.

### Reflections

From a high level perspective, this is more of a deep learning paper than a computer vision or natural language paper. That is perfectly fine since the key strength of attention is the fact that it has potentially very wide applicability.
One can imagine using this sort of mechanism for tasks such as saliency detection, object detection and visual language grounding, and these are only in computer vision. An efficient implementation of this could also be used for tasks such as Learning From Demonstration in robotics, amongst others.
The soft attention mechanism seems more appealing since there is less information discarded and no strong assumptions made about where the attention should be focused, unlike hard attention. The extremely easy visualization capabilities of either technique that we are granted for free make for icing on the cake.
From an implementation perspective, this model has many smaller parts. The heavy VGG network along with an LSTM and 3 MLPs make for a non-trivial system. An analysis of using a lighter network such as Inception or ResNet would make a strong case for deploying this framework in real world applications and on embedded devices.
We have already started seeing the vast adoption of visual attention, to the point that has become incremental in nature to incorporate attention to improve performance. This goes to illustrate the power of this technique.