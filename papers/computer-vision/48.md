## Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding

#### Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach

In this paper, the authors propose using an outer product (bilinear pooling) of the visual and text embeddings of the inputs for the tasks of VQA and Visual Grounding, in order to improve the interactions between these embeddings over simpler elementwise sums/products and concatenations in the hope of learning more semantically meaningful joint embeddings.
Faced with the challenge of extremely larger parameter sizes and slow performance due to the outer product, the authors propose the use of a compact formulation of the bilinear pooling by:
- Randomly projecting the feature vectors from each modality into a lower dimension space using the Sketch Count technique.
- Performing the outer-product in FFT space to avoid actual outer product calculations (which leads to a simple elementwise products in FFT space).

A large number of ablation studies are provided based on prior state-of-the-art techniques, and on multiple datasets and tasks (VQA, Visual Genome and Visual7W). Different architectures are proposed for each of the two tasks to leverage the sort of output required. The authors empirically show that their model outperforms all other state of the art models and is generalizable to different tasks while being easy to formulate and incorporate into existing architectures.

##### Strengths

1. The MCB is an elegantly framed technique that is easy to understand both conceptually and mathematically.
2. Excellent ablation studies illustrate the merit of this approach.
3. The use of MCB in 2 different but related tasks demonstrates its generalizability for different tasks and domains.
4. The MCB is easily extensible to more than 2 modalities making it a good technique to use for additional/different sets of input domains.

##### Weaknesses

1. The gains on the multiple choice VQA task do not seem quite significant (0.5% improvement), especially since comparing to other techniques, they each had at least 3% points of improvement over their previous state-of-the-art approaches.
2. No efficiency analysis in terms of training and inference. While the MCB is supposed to tackle both these issues, an analysis of the same would have gone a long way in making this paper more "complete".
3. The second MCB layer in the multiple attention settup for VQA open-ended, seems to be conveniently ignored in the ablation results. Despite the author's claims that the various models have the same parameter budget, we are not certain if the second MCB layer is providing an advantage since the model is now priming itself on the text embedding twice, once during attention and once during actual prediction.
4. The MCB model is given an unfair advantage by augmenting the VQA training dataset with training samples from Visual Genome. The paper is not clear if they did the same for all the other comparison models, hence it is unfair at face value.



##### Most Interesting Thought

It would be interesting to apply this technique to Vision+Language+Audio. Aytar et. al. released a dataset with their "See, Hear & Read" paper and thus the next big thing would be too see how well the MCB pooling deals with multiple modalities. Currently the authors try to learn a shared embedding, but using MCB may prove to be more efficient.
