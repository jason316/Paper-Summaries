## Deep Residual Learning for Image Recognition. 

#### Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. CVPR 2016.

In this paper, the authors propose learning a residual mapping F(x) = H(x) - x, instead of the unreferenced, underlying mapping H(x) in order to combat the degradation problem experienced by very deep networks. They propose a simple modification of allowing a shortcut connection between stacks of at least 2 layers which simulates the residual mapping without adding any more parameters. More particularly, they introduce a bottleneck architecture where a layer is now a 3x3 convolution prepended and appended with 1x1 convolutions so that the middle 3x3 layer is now a bottleneck. They discuss their architecture changes in designing networks of depths 34 layers, 50 layers, 101 layer and 152 layers and show results on ImageNet ILSVRC2015 and CIFAR-10 with the baseline being similar nets without the shortcut connections.

Shortcut Connections:
VGG16/19 uses padding of size 1 to maintain the same input-output dimensionality during convolution and and use max pooling over 2x2 windows which leads to the doubling of the number of channels or "increase in dimensions". Thus where max pooling does not occur, we can directly apply the identity connection. Where max pooling does occur, we apply the zero padding or a projection of the input.

I also managed to find this very cool visualization of the ResNet 152 on the github page of the 1st author: http://ethereon.github.io/netscope/#/gist/d38f3e6091952b45198b
The website also has visualizations of other models.