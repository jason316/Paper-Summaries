## Context-aware Captions from Context-agnostic Supervision

#### Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, Gal Chechik. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017

In this work, the authors propose an inference time formulation and technique to model pragmatics in the generated text from a RNN based language model. Two tasks are defined: 1) justify why an image is assigned to one class and not to another similar class but semantically different at a fine-grained level, and 2) generate a caption for a target image such that the caption is sufficiently discriminative from another semantically similara image. The authors present an alternative formulation to Andreas and Klein such that there is no need to train a separate discriminator model like in a GAN based framework, instead utilizing only the generator. The formulation takes advantage of the sampling distribution of the generative model a.k.a. the speaker, to sample output probabilities from both the target and distractor class or image, based on the task, and maximizing the final "introspective listener" objective such that the model is forced to output words that are semantically linked to the target class and image but also learns to avoid words from the distractor class/image. This leads to a model that can be trained on context-free caption data but can now be used to generative context-aware captions, a powerful idea in terms of scalability.

To validate their results on justification, the authors collect a new dataset, CUB-Justify, based on the CUB-200-2011 dataset. The collected dataset is only used during validation to find optimal lambda values (the tradeoff between faithful caption and discriminativeness), and testing. For the discriminative captioning task, the authors present a 2 annotation forced choice framework and perform human studies to evaluate the discriminative capabilities of the generated captions. Their final results demonstrate the effectiveness of their technique, significantly outperforming baseline approaches.

##### Strengths

1. This technique is used at inference time and hence there is no need to train a listener model. That is the biggest win for this model and allows existing systems to gracefully scale, even to new concepts.
2. The introspective speaker approach with the emitter-suppressor beam search is highly efficient in terms of computation since the search space is well-constrained on a local word levelby the introspective speaker formulation rather than utilizing a variational bayesian sampling on sentences such as the one used by Andreas and Klein.
3. Definition of new vision tasks that "go beyond captioning". This paper could spearhead research into language models that are self-explaining and more easily human-interpretable.

##### Weaknesses

1. The crowd sourcing for the CUB-Justify dataset seems very open ended and susceptible to worker bias and mistakes. I am assuming by worker they mean AMT workers (another glaring fault), and not ornithology hobbyists or experts who would be able to better justify the differences between fine grained categories. It is a well known issue in crowd sourcing that some workers can be unreliable and the open nature of caption collection does not enforce sufficient control. I have a suggestion to remedy this, but not writing it here for the sake of conciseness.
2. Use of different models for the 2 tasks. The justification task uses the attention based model whereas the discriminative captioning task uses the NIC model, with no justification made for this choice. Moreover, the authors reuse hyperparameters from the former model on the latter model without experimental validation, thus weakening their experimental analysis on the second task.



##### Most Interesting Thought

The CUB-Justify dataset needs to be re-evaluated. The dataset is open to flaws in the crowd sourcing pipeline and they should be corrected for research in this area to proceed in a scientific manner.
An interesting thought is to see how this approach works on different levels of hierarchy. For example, in Imagenet, you have categories such as animal, which have sub-categories dog, fox and wolf, and they each have their own subcategories. This would be a nice experiment to conduct to see just how far down the tree (or might I say rabbit hole), can this model successfully go, such as german shepherd vs australian shepherd, artic wolf vs timber wolf, dog vs fox, wolf vs dog and so on (there might be deeper hierarchies that I don't remember off the top of my head).
Finally, it would also be nice to see a comparative analysis on different datasets each with their own objective, such as indoor scenes, outdoor scenes, action datasets, etc. This may not lead to a tier-1 conference paper (maybe tier 2?), but would make for a valuable survey paper in this space, especially for researchers from other domains.