## Exploring Nearest Neighbor Approaches for Image Captioning. 

#### Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C Lawrence Zitnick. arXiv, 2015.

The authors provide a baseline, exploratory analysis of nearest neighbor approaches for the task of image captioning on the MS COCO dataset. They use 3 types of features, the handcrafted GIST descriptor, the deep features from VGG-16 FC7, and deep features from VGG-16 FC7 fine-tuned to perform classification on 1000 most common word in the captions. The novelty of their approach is the "consensus caption" which is the caption that best describes most of the k nearest neighbor images. To quantify best, they use similarity metrics such as CIDEr, BLEU and METEOR. Quantitative results show that NN approaches using deep features perform as well as state-of-the-art generative approaches, however, human evaluation via crowdsourcing gives contrasting results, highlighting the need for better automated evaluation metrics.

Discussion:
1. The authors bin the test images based on their mean distance to their 50 nearest neighbors in order to capture the degrees of visual similarity. Will doing this when performing human evaluation provide better insight? E.g. if a particular image stands out as very diverse from the training set, a human would still be easily able to accurately caption the image, given enough knowledge of the world, but the NN approach would fail simply because none of the NNs of that query image have suitable captions it can borrow from. 
This is alluded to in the discussion section but not directly touched upon.

2. I am assuming when finding the optimal K and M, the authors do NN using all 3 feature spaces and evaluate the best K and M across the feature spaces (though they don't elaborate their approach, which is somewhat confusing). Would different K and M values perform differently for different feature spaces? If they evaluation of optimal K and M could be explained, that would really help in answering this question.